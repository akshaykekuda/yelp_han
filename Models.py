# -*- coding: utf-8 -*-
"""Models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7fMYaCzIFXjdod6uObHyVH34szYMIXb
"""

from __future__ import unicode_literals, print_function, division
import torch
import torch.nn as nn
import numpy as np
from torch import optim
import torch.nn.functional as F
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class FCN(nn.Module):
    def __init__(self, input_size, dropout_rate):
        super(FCN, self).__init__()
        self.fcn = nn.Sequential(
            nn.Linear(input_size, 10),
            nn.Tanh(),
            nn.Dropout(dropout_rate),
            nn.Linear(10, 2),
            # nn.Tanh()
        )

    def forward(self, x):
        output = self.fcn(x)
        return output


class EncoderRNN(nn.Module):

    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, dropout_rate):
        super(EncoderRNN, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=1)
        self.embedding.load_state_dict({'weight': weights_matrix})

        self.hidden_size = hidden_size
        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True, bidirectional=True)
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, *_):
        embed_output = self.embedding(inputs)
        embed_output = torch.mean(embed_output, dim=2, keepdim=True).squeeze(2)
        output, hidden = self.gru(embed_output)
        output = output[:, -1, :]
        output = self.fcn(output)
        return output, None


class LSTMAttention(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, dropout_rate):
        super(LSTMAttention, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=1)
        self.embedding.load_state_dict({'weight': weights_matrix})
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)

        self.attn = nn.Sequential(
            nn.Linear(2 * hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Tanh()
        )
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, *_):
        embed_output = self.embedding(inputs)
        embed_output = torch.mean(embed_output, dim=2, keepdim=True).squeeze(2)
        output, hidden = self.lstm(embed_output)
        attn_weights = self.attn(output)
        attn_scores = F.softmax(attn_weights, 1)
        out = torch.bmm(output.transpose(1, 2), attn_scores).squeeze(2)
        logits = self.fcn(out)
        return logits, attn_scores.squeeze(2)


class GRUAttention(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, dropout_rate):
        super(GRUAttention, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=1)
        self.embedding.load_state_dict({'weight': weights_matrix})
        self.hidden_size = hidden_size
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)
        self.attn = nn.Sequential(
            nn.Linear(2 * hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Tanh()
        )
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, lens, trans_pos_indices, _):
        embed_output = self.embedding(inputs)
        # print(embed_output)
        attn_mask = trans_pos_indices == 0
        review_lens = (~attn_mask).sum(dim=1).cpu()
        embed_output = torch.mean(embed_output, dim=2, keepdim=True).squeeze(2)
        pck_seq = torch.nn.utils.rnn.pack_padded_sequence(embed_output, review_lens, batch_first=True, enforce_sorted=False)
        output_pckd, hidden = self.gru(pck_seq)
        output, review_lens = torch.nn.utils.rnn.pad_packed_sequence(output_pckd, batch_first=True, padding_value=0)
        # output, hidden = self.gru(embed_output)
        attn_weights = self.attn(output)
        ## mask weights
        attn_weights_masked = attn_weights.masked_fill(attn_mask.unsqueeze(2), value=-np.inf)
        # attn_weights = attn_mask.unsqueeze(2) * attn_weights
        attn_scores = F.softmax(attn_weights_masked, 1)
        out = torch.bmm(output.transpose(1, 2), attn_scores).squeeze(2)
        logits = self.fcn.forward(out)
        return logits, attn_scores.squeeze(2)


class HAN(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, dropout_rate):
        super(HAN, self).__init__()
        self.word_attention = WordAttention(vocab_size, embedding_size, hidden_size, weights_matrix)
        self.sentence_attention = SentenceAttention(2 * hidden_size, hidden_size)
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, lens, review_pos_indices, word_pos_indices):
        att1 = self.word_attention.forward(inputs, word_pos_indices)
        att2, sentence_att_scores = self.sentence_attention.forward(att1, review_pos_indices)
        logits = self.fcn.forward(att2)
        return logits, sentence_att_scores


class SentenceAttention(nn.Module):
    def __init__(self, sentence_embedding_size, hidden_size):
        super(SentenceAttention, self).__init__()
        self.lstm = nn.LSTM(sentence_embedding_size, hidden_size, batch_first=True, bidirectional=True)
        self.attn = nn.Sequential(
            nn.Linear(2 * hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Tanh()
        )

    def forward(self, inputs, positional_indices):
        padding_mask = positional_indices == 0
        review_lens = (~padding_mask).sum(dim=1).cpu()
        pck_seq = torch.nn.utils.rnn.pack_padded_sequence(inputs, review_lens, batch_first=True, enforce_sorted=False)
        output_pckd, hidden = self.lstm(pck_seq)
        output, review_lens = torch.nn.utils.rnn.pad_packed_sequence(output_pckd, batch_first=True, padding_value=0)
        # output, hidden = self.lstm(inputs)
        attn_weights = self.attn(output)
        attn_weights_masked = attn_weights.masked_fill(padding_mask.unsqueeze(2), value=-np.inf)
        attn_scores = F.softmax(attn_weights_masked, 1)
        out = torch.bmm(output.transpose(1, 2), attn_scores).squeeze(2)
        return out, attn_scores.squeeze(2)


class WordAttention(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, ):
        super(WordAttention, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=1)
        self.embedding.load_state_dict({'weight': weights_matrix})
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True, bidirectional=True)
        self.attn = nn.Sequential(
            nn.Linear(2 * hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Tanh()
        )

    def forward(self, inputs, positional_indices):
        embed_output = self.embedding(inputs)
        embed_output_cat = embed_output.view(-1, *embed_output.size()[2:])
        padding_mask = positional_indices == 0
        sent_lens = (~padding_mask).sum(dim=-1).view(-1).cpu()
        pck_seq = torch.nn.utils.rnn.pack_padded_sequence(embed_output_cat, sent_lens, batch_first=True, enforce_sorted=False)
        word_out_pckd, word_hidden = self.lstm(pck_seq)
        word_out, sent_lens = torch.nn.utils.rnn.pad_packed_sequence(word_out_pckd, batch_first=True, padding_value=0)
        # word_out, hidden = self.lstm(embed_output_cat)
        attn_weights = self.attn(word_out)
        ## mask weights
        ##change to use pos indices
        attn_weights = attn_weights.masked_fill(padding_mask.view(-1, *padding_mask.size()[2:]).unsqueeze(dim=2), value=-np.inf)
        attn_scores = F.softmax(attn_weights, 1)
        # attn_scores = torch.nan_to_num(attn_scores)
        sentence_embedding = torch.sum(word_out * attn_scores, 1)
        return sentence_embedding.reshape(*inputs.size()[0:2], -1)


class HSAN(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, max_trans_len,
                 max_sent_len, num_heads, dropout_rate):
        super(HSAN, self).__init__()
        self.word_attention = WordAttention(vocab_size, embedding_size, hidden_size, weights_matrix)
        self.sentence_self_attention = SentenceSelfAttention(2 * hidden_size, num_heads, max_trans_len, dropout_rate)
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, lens, trans_pos_indices, word_pos_indices):
        att1 = self.word_attention.forward(inputs, word_pos_indices)
        att2, sentence_att_scores = self.sentence_self_attention.forward(att1, trans_pos_indices)
        output = self.fcn.forward(att2)
        return output, sentence_att_scores


class HS2AN(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, weights_matrix, max_trans_len,
                 max_sent_len, num_heads, dropout_rate):
        super(HS2AN, self).__init__()
        self.word_self_attention = WordSelfAttention(vocab_size, embedding_size, 2 * hidden_size, weights_matrix,
                                                     max_sent_len, num_heads, dropout_rate)
        self.sentence_self_attention = SentenceSelfAttention(2 * hidden_size, num_heads, max_trans_len, dropout_rate)
        self.fcn = FCN(2*hidden_size, dropout_rate)

    def forward(self, inputs, lens, trans_pos_indices, word_pos_indices):
        att1 = self.word_self_attention.forward(inputs, word_pos_indices)
        att2, sentence_att_scores = self.sentence_self_attention.forward(att1, trans_pos_indices)
        output = self.fcn.forward(att2)
        return output, sentence_att_scores


class SentenceSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, max_trans_len, dropout_rate):
        super(SentenceSelfAttention, self).__init__()
        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)
        self.position_encoding = nn.Embedding(max_trans_len, embed_dim, padding_idx=0)

    def forward(self, inputs, positional_indices):
        # positional_encoding = self.position_encoding(positional_indices)
        # att_in = inputs + positional_encoding
        padding_mask = positional_indices == 0
        att_in = inputs
        query = key = value = att_in
        attn_output, attn_output_weights = self.multihead_attn(query, key, value, key_padding_mask=padding_mask)
        # mask_for_pads = (~padding_mask).unsqueeze(-1).expand(-1, -1, attn_output.size(-1))
        # attn_output *= mask_for_pads
        attn_output = torch.mean(attn_output, dim=1, keepdim=False)
        return attn_output, attn_output_weights.squeeze(2)


class WordSelfAttention(nn.Module):
    def __init__(self, vocab_size, embedding_size, out_dim, weights_matrix, max_sent_len, num_heads, dropout_rate):
        super(WordSelfAttention, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=1)
        self.embedding.load_state_dict({'weight': weights_matrix})
        self.multihead_attn = nn.MultiheadAttention(embedding_size, dropout=dropout_rate, num_heads=num_heads, batch_first=True)
        self.ffn = nn.Linear(embedding_size, out_dim)
        self.position_encoding = nn.Embedding(max_sent_len, embedding_size, padding_idx=0)

    def forward(self, inputs, positional_indices):
        embed_output = self.embedding(inputs)
        embed_output_cat = embed_output.view(-1, *embed_output.size()[2:])
        # position_encoding = self.position_encoding(positional_indices)
        # position_encoding_cat = position_encoding.view(-1, *position_encoding.size()[2:])
        padding_mask = (positional_indices == 0).view(-1, *positional_indices.size()[2:])
        attn_in = embed_output_cat
        query = key = value = attn_in
        attn_output, attn_output_weights = self.multihead_attn(query, key, value, key_padding_mask=padding_mask)
        #force pad attention outputs
        # padding_mask = (inputs == 1).view(-1, *inputs.size()[2:])
        # mask_for_pads = (~padding_mask).unsqueeze(-1).expand(-1, -1, attn_output.size(-1))
        # attn_output *= mask_for_pads
        sent_embedding = torch.mean(attn_output, dim=1, keepdim=False)
        sent_embedding = sent_embedding.reshape(*inputs.size()[0:2], -1)
        # sent_embedding = torch.nan_to_num(sent_embedding)
        sent_embedding = self.ffn(sent_embedding)
        return sent_embedding



