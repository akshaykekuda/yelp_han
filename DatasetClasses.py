# -*- coding: utf-8 -*-
"""DatasetClasses

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WGuilwMSn2xsfnKuw4yy27BkIlL_8A-h
"""

from torch.utils.data import Dataset
import numpy as np
import pandas as pd
import torch
from collections import Counter
from torchtext.vocab import vocab
from torchtext.data.utils import get_tokenizer
from nltk.tokenize import sent_tokenize

scoring_criteria = ['Greeting', 'Professionalism', 'Confidence',
                    'Cross Selling', 'Retention', 'Creates Incentive', 'Product Knowledge',
                    'Documentation', 'Education', 'Processes', 'Category', 'CombinedPercentileScore']
class YelpDataset(Dataset):
    """Yelp dataset."""

    def __init__(self, file_name):
        """
        Args:
            file_name: The json file to make the dataset from
        """
        self.df = pd.read_json(file_name, lines=True)
        self.max_review_len, self.max_sent_len = self.get_max_len(self.df)

        word_tokenizer = get_tokenizer('basic_english')
        
        binary_cat = []
        counter = Counter()
        reviews = []

        #Create target class for each review, build vocab
        for index, row in self.df.iterrows():
            binary_cat.append(row['category'])
            sentences = sent_tokenize(row['text'])
            sentences = [s.replace('.', '') for s in sentences]
            reviews.append([sent for sent in sentences if len(sent) > 0])
            for i in range(len(sentences)):
              words = word_tokenizer(sentences[i])
              counter.update(words)

        self.vocab = vocab(counter)
        self.vocab.insert_token('<cls>', 0)
        self.vocab.insert_token('<pad>', 0)
        self.vocab.insert_token('<UNK>', 0)
        self.vocab.set_default_index(0)
        self.df['category'] = binary_cat
        self.df['text'] = reviews

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        category = self.df.iloc[idx, 0]
        text = self.df.iloc[idx, 1]
        sample = {'category': category, 'text': text}
        return sample

    def get_vocab(self):
      return self.vocab

    def get_max_len(self, df):
        def fun(sent):
            return len(sent.split())
        max_review_len = np.max(df.text.apply(lambda x: len(x.split("."))))
        max_sent_len = np.max(df.text.apply(lambda x: max(map(fun, x.split('.')))))
        return max_review_len, max_sent_len

    def save_vocab(self, path):
        import pickle
        output = open(path, 'wb')
        pickle.dump(self.vocab, output)
        output.close()
